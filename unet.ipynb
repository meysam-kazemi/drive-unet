{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U_i0GXtOkdtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df7e518-f16b-4865-99fd-d254d105f758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmixHCO7xMAj",
        "outputId": "5d14cdf4-6391-4468-e35c-1a301739d24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/RETINAL\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/RETINAL/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mx0PGIEy9ES",
        "outputId": "77f45e94-5449-479b-bd04-8b0650fa6ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "augment.ipynb  \u001b[0m\u001b[01;34mfiles\u001b[0m/  \u001b[01;34mmodels\u001b[0m/  \u001b[01;34mnew_data\u001b[0m/  \u001b[01;34mtest\u001b[0m/  test.zip  \u001b[01;34mtraining\u001b[0m/  training.zip  unet.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from glob import glob\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import cv2\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "rFq894oTxN-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "\n",
        "\"\"\" Seeding the randomness. \"\"\"\n",
        "def seeding(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\"\"\" Create a directory. \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\"\"\" Calculate the time taken \"\"\"\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "Bvn8SEgBkwF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "\n",
        "class DriveDataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path):\n",
        "\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.n_samples = len(images_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
        "        image = image/255.0 ## (512, 512, 3)\n",
        "        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n",
        "        image = image.astype(np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        \"\"\" Reading mask \"\"\"\n",
        "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = mask/255.0   ## (512, 512)\n",
        "        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "Yy2gyLkpkwKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "\n",
        "        return Dice_BCE"
      ],
      "metadata": {
        "id": "7mjwc5dTkwPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = conv_block(in_c, out_c)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        p = self.pool(x)\n",
        "\n",
        "        return x, p\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "        self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.up(inputs)\n",
        "        x = torch.cat([x, skip], axis=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class build_unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        self.e1 = encoder_block(3, 64)\n",
        "        self.e2 = encoder_block(64, 128)\n",
        "        self.e3 = encoder_block(128, 256)\n",
        "        self.e4 = encoder_block(256, 512)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        self.b = conv_block(512, 1024)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        self.d1 = decoder_block(1024, 512)\n",
        "        self.d2 = decoder_block(512, 256)\n",
        "        self.d3 = decoder_block(256, 128)\n",
        "        self.d4 = decoder_block(128, 64)\n",
        "\n",
        "        \"\"\" Classifier \"\"\"\n",
        "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        s1, p1 = self.e1(inputs)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        b = self.b(p4)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "\n",
        "        outputs = self.outputs(d4)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "x = torch.randn((2, 3, 512, 512))\n",
        "model = build_unet()\n",
        "y = model(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZqG8dMgkwT3",
        "outputId": "223246d5-005f-49ec-aea0-38d26c7ccd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained Model\n",
        "model.load_state_dict(torch.load(\"models/unet.pth\"), strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCR0t295901E",
        "outputId": "b5c1f305-c378-46c0-91a4-2d0c7cd7dd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss/(len(loader)+1e-5)\n",
        "    return epoch_loss\n"
      ],
      "metadata": {
        "id": "rBrkenQ290uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Seeding \"\"\"\n",
        "seeding(42)\n",
        "\n",
        "\"\"\" Directories \"\"\"\n",
        "create_dir(\"files\")\n",
        "\n",
        "\"\"\" Load dataset \"\"\"\n",
        "train_x = sorted(glob(\"./new_data/train/image/*\"))\n",
        "train_y = sorted(glob(\"./new_data/train/mask/*\"))\n",
        "\n",
        "valid_x = sorted(glob(\"./new_data/test/image/*\"))\n",
        "valid_y = sorted(glob(\"./new_data/test/mask/*\"))\n",
        "\n",
        "data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
        "print(data_str)\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "size = (H, W)\n",
        "batch_size = 4\n",
        "num_epochs = 150\n",
        "lr = 1e-4\n",
        "checkpoint_path = \"files/checkpoint.pth\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMYTHQ0J90iI",
        "outputId": "cafe99bf-8299-41a9-b24d-9723bf702092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size:\n",
            "Train: 1600 - Valid: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Dataset and loader \"\"\"\n",
        "train_dataset = DriveDataset(train_x, train_y)\n",
        "valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model = build_unet()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "loss_fn = DiceBCELoss()\n"
      ],
      "metadata": {
        "id": "t1OiYTdp-wnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23566876-bfa6-49e4-dfa9-f85d3d970372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = evaluate(model, train_loader, loss_fn, device)\n",
        "print(f\"evaluation loss for new train data : {eval_loss}\")"
      ],
      "metadata": {
        "id": "uSk5Pab1-wz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQrNkPMJBz5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_dir(\"./outputs/\")\n",
        "create_dir(\"./outputs/unet\")\n",
        "create_dir(\"./outputs/unet/image\")\n",
        "create_dir(\"./outputs/unet/result\")"
      ],
      "metadata": {
        "id": "7PSIOEuyB0_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, _ in train_loader:\n",
        "      if i== 20:\n",
        "        break\n",
        "      for j in range(4):\n",
        "        cv2.imwrite(f\"outputs/unet/image/{i}-{j}.png\", np.array(x[j]).transpose([1,2,0]))\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y_pred = model(x)\n",
        "      for j in range(4):\n",
        "        cv2.imwrite(f\"outputs/unet/result/{i}-{j}.png\", np.array(y_pred[j].to(\"cpu\")).transpose([1,2,0]))\n",
        "\n",
        "      i += 1"
      ],
      "metadata": {
        "id": "cD6uQd7R-w6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_parse(mask):\n",
        "  mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
        "  mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
        "  return mask\n",
        "\n",
        "def renorm(mask):\n",
        "  mn = mask.min()\n",
        "  mx = mask.max()\n",
        "  mask = 255*(mask-mn)/(mx-mn)\n",
        "  return mask\n",
        "\n",
        "time_taken = []\n",
        "model.eval()\n",
        "for i, x  in enumerate(train_x):\n",
        "  if i==20:\n",
        "    break\n",
        "\n",
        "  \"\"\" Extract the name \"\"\"\n",
        "  name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "  \"\"\" Reading image \"\"\"\n",
        "  image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n",
        "  cv2.imwrite(f\"./outputs/unet/image/{i}.png\", image)\n",
        "  ## image = cv2.resize(image, size)\n",
        "  x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n",
        "  x = x/255.0\n",
        "  x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n",
        "  x = x.astype(np.float32)\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      \"\"\" Prediction and Calculating FPS \"\"\"\n",
        "      start_time = time.time()\n",
        "      pred_y = model(x)\n",
        "      # pred_y = torch.sigmoid(pred_y)\n",
        "      total_time = time.time() - start_time\n",
        "      time_taken.append(total_time)\n",
        "\n",
        "      pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
        "      pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
        "      print(\"1:\", pred_y.min(), pred_y.max())\n",
        "      pred_y = renorm(pred_y)\n",
        "      pred_y = pred_y > 0.5\n",
        "      pred_y = np.array(pred_y*255, dtype=np.uint8)\n",
        "      print('2: ', pred_y.min(), pred_y.max())\n",
        "\n",
        "  \"\"\" Saving masks \"\"\"\n",
        "  pred_y = mask_parse(pred_y)\n",
        "\n",
        "  cv2.imwrite(f\"outputs/unet/result/{i}.png\", pred_y)\n",
        "\n",
        "print(\"mean time: \", np.mean(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x_sdtFe-w_k",
        "outputId": "67cdb3b1-7d27-4c6b-8745-6109417cd2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: -0.12291881 -0.113077626\n",
            "2:  0 255\n",
            "1: -0.120862246 -0.11354889\n",
            "2:  0 255\n",
            "1: -0.12087087 -0.11438538\n",
            "2:  0 255\n",
            "1: -0.120876946 -0.11330003\n",
            "2:  0 255\n",
            "1: -0.12083252 -0.11349603\n",
            "2:  0 255\n",
            "1: -0.122946344 -0.1130783\n",
            "2:  0 255\n",
            "1: -0.12293338 -0.11272229\n",
            "2:  0 255\n",
            "1: -0.12304406 -0.11258076\n",
            "2:  0 255\n",
            "1: -0.12310955 -0.11316435\n",
            "2:  0 255\n",
            "1: -0.12226122 -0.111906335\n",
            "2:  0 255\n",
            "1: -0.12087742 -0.11273057\n",
            "2:  0 255\n",
            "1: -0.12085917 -0.11405827\n",
            "2:  0 255\n",
            "1: -0.12240405 -0.11197403\n",
            "2:  0 255\n",
            "1: -0.12287896 -0.11288087\n",
            "2:  0 255\n",
            "1: -0.122164235 -0.111101426\n",
            "2:  0 255\n",
            "1: -0.122861005 -0.11310038\n",
            "2:  0 255\n",
            "1: -0.12297594 -0.11273643\n",
            "2:  0 255\n",
            "1: -0.12104834 -0.11452871\n",
            "2:  0 255\n",
            "1: -0.12292545 -0.11258597\n",
            "2:  0 255\n",
            "1: -0.12083259 -0.11346763\n",
            "2:  0 255\n",
            "mean time:  0.007657849788665771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8959bHV-xEB",
        "outputId": "46498fcb-3947-487c-b7da-5518cf6c9a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-LwSIiy-xIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6-Do88K-xM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TM0U-mC3-xRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NinVl7M2-xVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQkx2ixg-xan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "\n",
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, dtype=torch.float32)\n",
        "        y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    return epoch_loss\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss/(len(loader)+1e-5)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Seeding \"\"\"\n",
        "seeding(42)\n",
        "\n",
        "\"\"\" Directories \"\"\"\n",
        "create_dir(\"files\")\n",
        "\n",
        "\"\"\" Load dataset \"\"\"\n",
        "train_x = sorted(glob(\"./new_data/train/image/*\"))\n",
        "train_y = sorted(glob(\"./new_data/train/mask/*\"))\n",
        "\n",
        "valid_x = sorted(glob(\"./new_data/test/image/*\"))\n",
        "valid_y = sorted(glob(\"./new_data/test/mask/*\"))\n",
        "\n",
        "data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
        "print(data_str)\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "size = (H, W)\n",
        "batch_size = 4\n",
        "num_epochs = 150\n",
        "lr = 1e-4\n",
        "checkpoint_path = \"files/checkpoint.pth\"\n",
        "\n",
        "\"\"\" Dataset and loader \"\"\"\n",
        "train_dataset = DriveDataset(train_x, train_y)\n",
        "valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model = build_unet()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "loss_fn = DiceBCELoss()\n",
        "\n",
        "\"\"\" Training the model \"\"\"\n",
        "best_valid_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
        "    # valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
        "\n",
        "    \"\"\" Saving the model \"\"\"\n",
        "    if train_loss < best_valid_loss:\n",
        "        data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {train_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "        print(data_str)\n",
        "\n",
        "        best_valid_loss = train_loss\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "    data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "    # data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
        "    print(data_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJkOMBUGkwYY",
        "outputId": "6d8ebf31-42c6-4709-fad5-6ff20f18b400"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Size:\n",
            "Train: 1600 - Valid: 0\n",
            "\n",
            "Valid loss improved from inf to 0.8432. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 01 | Epoch Time: 7m 19s\n",
            "\tTrain Loss: 0.843\n",
            "\n",
            "Valid loss improved from 0.8432 to 0.5511. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 02 | Epoch Time: 7m 12s\n",
            "\tTrain Loss: 0.551\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uu8MPRU31xpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rER6yBMFGsV8",
        "outputId": "4313346d-7fa8-402b-a26e-c0e5ac29936f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1e-11"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "import os, time\n",
        "from operator import add\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "\n",
        "from model import build_unet\n",
        "from utils import create_dir, seeding\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Ground truth \"\"\"\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "    y_true = y_true.reshape(-1)\n",
        "\n",
        "    \"\"\" Prediction \"\"\"\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "\n",
        "    score_jaccard = jaccard_score(y_true, y_pred)\n",
        "    score_f1 = f1_score(y_true, y_pred)\n",
        "    score_recall = recall_score(y_true, y_pred)\n",
        "    score_precision = precision_score(y_true, y_pred)\n",
        "    score_acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc]\n",
        "\n",
        "def mask_parse(mask):\n",
        "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
        "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
        "    return mask\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    seeding(42)\n",
        "\n",
        "    \"\"\" Folders \"\"\"\n",
        "    create_dir(\"results\")\n",
        "\n",
        "    \"\"\" Load dataset \"\"\"\n",
        "    test_x = sorted(glob(\"../new_data/test/image/*\"))\n",
        "    test_y = sorted(glob(\"../new_data/test/mask/*\"))\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    H = 512\n",
        "    W = 512\n",
        "    size = (W, H)\n",
        "    checkpoint_path = \"files/checkpoint.pth\"\n",
        "\n",
        "    \"\"\" Load the checkpoint \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = build_unet()\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    time_taken = []\n",
        "\n",
        "    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n",
        "        \"\"\" Extract the name \"\"\"\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n",
        "        ## image = cv2.resize(image, size)\n",
        "        x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n",
        "        x = x/255.0\n",
        "        x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n",
        "        x = x.astype(np.float32)\n",
        "        x = torch.from_numpy(x)\n",
        "        x = x.to(device)\n",
        "\n",
        "        \"\"\" Reading mask \"\"\"\n",
        "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
        "        ## mask = cv2.resize(mask, size)\n",
        "        y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)\n",
        "        y = y/255.0\n",
        "        y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)\n",
        "        y = y.astype(np.float32)\n",
        "        y = torch.from_numpy(y)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            \"\"\" Prediction and Calculating FPS \"\"\"\n",
        "            start_time = time.time()\n",
        "            pred_y = model(x)\n",
        "            pred_y = torch.sigmoid(pred_y)\n",
        "            total_time = time.time() - start_time\n",
        "            time_taken.append(total_time)\n",
        "\n",
        "\n",
        "            score = calculate_metrics(y, pred_y)\n",
        "            metrics_score = list(map(add, metrics_score, score))\n",
        "            pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
        "            pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
        "            pred_y = pred_y > 0.5\n",
        "            pred_y = np.array(pred_y, dtype=np.uint8)\n",
        "\n",
        "        \"\"\" Saving masks \"\"\"\n",
        "        ori_mask = mask_parse(mask)\n",
        "        pred_y = mask_parse(pred_y)\n",
        "        line = np.ones((size[1], 10, 3)) * 128\n",
        "\n",
        "        cat_images = np.concatenate(\n",
        "            [image, line, ori_mask, line, pred_y * 255], axis=1\n",
        "        )\n",
        "        cv2.imwrite(f\"results/{name}.png\", cat_images)\n",
        "\n",
        "    jaccard = metrics_score[0]/len(test_x)\n",
        "    f1 = metrics_score[1]/len(test_x)\n",
        "    recall = metrics_score[2]/len(test_x)\n",
        "    precision = metrics_score[3]/len(test_x)\n",
        "    acc = metrics_score[4]/len(test_x)\n",
        "    print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f}\")\n",
        "\n",
        "    fps = 1/np.mean(time_taken)\n",
        "    print(\"FPS: \", fps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Wyiaa0wwkwcn",
        "outputId": "e97539c5-4d0f-48ec-bd40-d11e36e43443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8b6029e59fdc>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_unet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvxATSGlkwgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkZi4ayZkwkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XL29Ae9dkwoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nuJ3HgvYkws7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2rA2RjWkwxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}