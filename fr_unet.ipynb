{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ratWbIZ5pa",
        "outputId": "2ff45342-f5e9-48d8-d175-e22cf388b2cc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/RETINAL/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RDOGMgAXqtP",
        "outputId": "3cdf0e15-8bb9-4eff-9f2d-6f600a4a99bf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/19v_-z68QIom5bSkKsoiETI2xN8cPjPox/RETINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mx0PGIEy9ES",
        "outputId": "ebb5c7d5-4861-4ff3-b6a5-6fd17c8bf0a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "augment.ipynb  fr-unet.ipynb  \u001b[0m\u001b[01;34mnew_data\u001b[0m/  \u001b[01;34mtest\u001b[0m/     \u001b[01;34mtraining\u001b[0m/     unet-2.ipynb\n",
            "\u001b[01;34mfiles\u001b[0m/         \u001b[01;34mmodels\u001b[0m/        \u001b[01;34moutputs\u001b[0m/   test.zip  training.zip  unet.ipynb\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "q6TWytw97N0T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'model':{'type': 'FR_UNet'},\n",
        "    'loss':{'type': 'BCELoss'},\n",
        "    'optimizer':{'type': 'Adam','args':{'lr': 1e-4,'weight_decay': 1e-5}},\n",
        "    'lr_scheduler':{'type': 'CosineAnnealingLR','args':{'T_max': 40}},\n",
        "    'epochs':40,\n",
        "    'save_dir': 'saved/',\n",
        "    'val_per_epochs': 1,\n",
        "    'save_period': 1,\n",
        "    'tensorboard': True,\n",
        "    'threshold': 0.5,\n",
        "    'threshold_low': 0.3,\n",
        "    'DTI': False,\n",
        "    'amp': True,\n",
        "    'tta': False,\n",
        "    'CCC': False,\n",
        "    }"
      ],
      "metadata": {
        "id": "r3zsz7TZbIbm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS"
      ],
      "metadata": {
        "id": "ub0JXYCtcYTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helpers\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "\n",
        "def get_instance(module, name, config, *args):\n",
        "    return getattr(module, config[name]['type'])(*args, **config[name]['args'])\n",
        "\n",
        "\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "class Fix_RandomRotation(object):\n",
        "\n",
        "    def __init__(self, degrees=360, resample=False, expand=False, center=None):\n",
        "        self.degrees = degrees\n",
        "        self.resample = resample\n",
        "        self.expand = expand\n",
        "        self.center = center\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params():\n",
        "        p = torch.rand(1)\n",
        "\n",
        "        if p >= 0 and p < 0.25:\n",
        "            angle = -180\n",
        "        elif p >= 0.25 and p < 0.5:\n",
        "            angle = -90\n",
        "        elif p >= 0.5 and p < 0.75:\n",
        "            angle = 90\n",
        "        else:\n",
        "            angle = 0\n",
        "        return angle\n",
        "\n",
        "    def __call__(self, img):\n",
        "        angle = self.get_params()\n",
        "        return F.rotate(img, angle, self.resample, self.expand, self.center)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \\\n",
        "            '(degrees={0}'.format(self.degrees)\n",
        "        format_string += ', resample={0}'.format(self.resample)\n",
        "        format_string += ', expand={0}'.format(self.expand)\n",
        "        if self.center is not None:\n",
        "            format_string += ', center={0}'.format(self.center)\n",
        "        format_string += ')'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "def dir_exists(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def remove_files(path):\n",
        "    for root, dirs, files in os.walk(path, topdown=False):\n",
        "        for name in files:\n",
        "            os.remove(os.path.join(root, name))\n",
        "        for name in dirs:\n",
        "            os.rmdir(os.path.join(root, name))\n",
        "\n",
        "\n",
        "def read_pickle(path, type):\n",
        "    with open(file=path + f\"/{type}.pkl\", mode='rb') as file:\n",
        "        img = pickle.load(file)\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_pickle(path, type, img_list):\n",
        "    with open(file=path + f\"/{type}.pkl\", mode='wb') as file:\n",
        "        pickle.dump(img_list, file)\n",
        "\n",
        "\n",
        "def double_threshold_iteration(index,img, h_thresh, l_thresh, save=True):\n",
        "    h, w = img.shape\n",
        "    img = np.array(torch.sigmoid(img).cpu().detach()*255, dtype=np.uint8)\n",
        "    bin = np.where(img >= h_thresh*255, 255, 0).astype(np.uint8)\n",
        "    gbin = bin.copy()\n",
        "    gbin_pre = gbin-1\n",
        "    while(gbin_pre.all() != gbin.all()):\n",
        "        gbin_pre = gbin\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                if gbin[i][j] == 0 and img[i][j] < h_thresh*255 and img[i][j] >= l_thresh*255:\n",
        "                    if gbin[i-1][j-1] or gbin[i-1][j] or gbin[i-1][j+1] or gbin[i][j-1] or gbin[i][j+1] or gbin[i+1][j-1] or gbin[i+1][j] or gbin[i+1][j+1]:\n",
        "                        gbin[i][j] = 255\n",
        "\n",
        "    if save:\n",
        "        cv2.imwrite(f\"save_picture/bin{index}.png\", bin)\n",
        "        cv2.imwrite(f\"save_picture/gbin{index}.png\", gbin)\n",
        "    return gbin/255\n",
        "\n",
        "\n",
        "def recompone_overlap(preds, img_h, img_w, stride_h, stride_w):\n",
        "    assert (len(preds.shape) == 4)\n",
        "    assert (preds.shape[1] == 1 or preds.shape[1] == 3)\n",
        "    patch_h = preds.shape[2]\n",
        "    patch_w = preds.shape[3]\n",
        "    N_patches_h = (img_h - patch_h) // stride_h + 1\n",
        "    N_patches_w = (img_w - patch_w) // stride_w + 1\n",
        "    N_patches_img = N_patches_h * N_patches_w\n",
        "    assert (preds.shape[0] % N_patches_img == 0)\n",
        "    N_full_imgs = preds.shape[0] // N_patches_img\n",
        "    full_prob = np.zeros((N_full_imgs, preds.shape[1], img_h, img_w))\n",
        "    full_sum = np.zeros((N_full_imgs, preds.shape[1], img_h, img_w))\n",
        "    k = 0\n",
        "    for i in range(N_full_imgs):\n",
        "        for h in range((img_h - patch_h) // stride_h + 1):\n",
        "            for w in range((img_w - patch_w) // stride_w + 1):\n",
        "                full_prob[i, :, h * stride_h:(h * stride_h) + patch_h, w * stride_w:(w * stride_w) + patch_w] += preds[\n",
        "                    k]\n",
        "                full_sum[i, :, h * stride_h:(h * stride_h) + patch_h,\n",
        "                         w * stride_w:(w * stride_w) + patch_w] += 1\n",
        "                k += 1\n",
        "    assert (k == preds.shape[0])\n",
        "    assert (np.min(full_sum) >= 1.0)\n",
        "    final_avg = full_prob / full_sum\n",
        "    return final_avg"
      ],
      "metadata": {
        "id": "ZRbxQKVhcYNu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# losses\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, ignore_index=255, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.CE_loss = nn.CrossEntropyLoss(\n",
        "            reduction=reduction, ignore_index=ignore_index, weight=alpha)\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        logpt = self.CE_loss(output, target)\n",
        "        pt = torch.exp(-logpt)\n",
        "        loss = ((1 - pt) ** self.gamma) * logpt\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        return loss.sum()\n",
        "\n",
        "\n",
        "class BCELoss(nn.Module):\n",
        "    def __init__(self, reduction=\"mean\", pos_weight=1.0):\n",
        "        pos_weight = torch.tensor(pos_weight).cuda()\n",
        "        super(BCELoss, self).__init__()\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(\n",
        "            reduction=reduction, pos_weight=pos_weight)\n",
        "\n",
        "    def forward(self, prediction, targets):\n",
        "        return self.bce_loss(prediction, targets)\n",
        "\n",
        "\n",
        "class CELoss(nn.Module):\n",
        "    def __init__(self, weight=[1, 1], ignore_index=-100, reduction='mean'):\n",
        "        super(CELoss, self).__init__()\n",
        "        weight = torch.tensor(weight).cuda()\n",
        "        self.CE = nn.CrossEntropyLoss(\n",
        "            weight=weight, ignore_index=ignore_index, reduction=reduction)\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = self.CE(output, target.squeeze(1).long())\n",
        "        return loss\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-8):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        prediction = torch.sigmoid(prediction)\n",
        "        intersection = 2 * torch.sum(prediction * target) + self.smooth\n",
        "        union = torch.sum(prediction) + torch.sum(target) + self.smooth\n",
        "        loss = 1 - intersection / union\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CE_DiceLoss(nn.Module):\n",
        "    def __init__(self, reduction=\"mean\", D_weight=0.5):\n",
        "        super(CE_DiceLoss, self).__init__()\n",
        "        self.DiceLoss = DiceLoss()\n",
        "        self.BCELoss = BCELoss(reduction=reduction)\n",
        "        self.D_weight = D_weight\n",
        "\n",
        "    def forward(self, prediction, targets):\n",
        "        return self.D_weight * self.DiceLoss(prediction, targets) + (1 - self.D_weight) * self.BCELoss(prediction,\n",
        "                                                                                                       targets)"
      ],
      "metadata": {
        "id": "SHWSEbODcYCk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrices\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.initialized = False\n",
        "        self.val = None\n",
        "        self.avg = None\n",
        "        self.sum = None\n",
        "        self.count = None\n",
        "\n",
        "    def initialize(self, val, weight):\n",
        "        self.val = val\n",
        "        self.avg = val\n",
        "        self.sum = np.multiply(val, weight)\n",
        "        self.count = weight\n",
        "        self.initialized = True\n",
        "\n",
        "    def update(self, val, weight=1):\n",
        "        if not self.initialized:\n",
        "            self.initialize(val, weight)\n",
        "        else:\n",
        "            self.add(val, weight)\n",
        "\n",
        "    def add(self, val, weight):\n",
        "        self.val = val\n",
        "        self.sum = np.add(self.sum, np.multiply(val, weight))\n",
        "        self.count = self.count + weight\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return np.round(self.val, 4)\n",
        "\n",
        "    @property\n",
        "    def average(self):\n",
        "        return np.round(self.avg, 4)\n",
        "\n",
        "\n",
        "def get_metrics(predict, target, threshold=None, predict_b=None):\n",
        "    predict = torch.sigmoid(predict).cpu().detach().numpy().flatten()\n",
        "    if predict_b is not None:\n",
        "        predict_b = predict_b.flatten()\n",
        "    else:\n",
        "        predict_b = np.where(predict >= threshold, 1, 0)\n",
        "    if torch.is_tensor(target):\n",
        "        target = target.cpu().detach().numpy().flatten()\n",
        "    else:\n",
        "        target = target.flatten()\n",
        "    tp = (predict_b * target).sum()\n",
        "    tn = ((1 - predict_b) * (1 - target)).sum()\n",
        "    fp = ((1 - target) * predict_b).sum()\n",
        "    fn = ((1 - predict_b) * target).sum()\n",
        "    auc = roc_auc_score(target, predict)\n",
        "    acc = (tp + tn) / (tp + fp + fn + tn)\n",
        "    pre = tp / (tp + fp)\n",
        "    sen = tp / (tp + fn)\n",
        "    spe = tn / (tn + fp)\n",
        "    iou = tp / (tp + fp + fn)\n",
        "    f1 = 2 * pre * sen / (pre + sen)\n",
        "    return {\n",
        "        \"AUC\": np.round(auc, 4),\n",
        "        \"F1\": np.round(f1, 4),\n",
        "        \"Acc\": np.round(acc, 4),\n",
        "        \"Sen\": np.round(sen, 4),\n",
        "        \"Spe\": np.round(spe, 4),\n",
        "        \"pre\": np.round(pre, 4),\n",
        "        \"IOU\": np.round(iou, 4),\n",
        "    }\n",
        "\n",
        "\n",
        "def count_connect_component(predict, target, threshold=None, connectivity=8):\n",
        "    if threshold != None:\n",
        "        predict = torch.sigmoid(predict).cpu().detach().numpy()\n",
        "        predict = np.where(predict >= threshold, 1, 0)\n",
        "    if torch.is_tensor(target):\n",
        "        target = target.cpu().detach().numpy()\n",
        "    pre_n, _, _, _ = cv2.connectedComponentsWithStats(np.asarray(\n",
        "        predict, dtype=np.uint8)*255, connectivity=connectivity)\n",
        "    gt_n, _, _, _ = cv2.connectedComponentsWithStats(np.asarray(\n",
        "        target, dtype=np.uint8)*255, connectivity=connectivity)\n",
        "    return pre_n/gt_n"
      ],
      "metadata": {
        "id": "g4KiAThQclyl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "hn8lc8Cxc0dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "iDFJOD5Ec0UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl8Y1h-AdBx6",
        "outputId": "ac7f0fad-b1ea-4186-81b2-ff6050e4e603"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from timm.models.layers import trunc_normal_\n",
        "\n",
        "class InitWeights_He(object):\n",
        "    def __init__(self, neg_slope=1e-2):\n",
        "        self.neg_slope = neg_slope\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if isinstance(module, nn.Conv3d) or isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.ConvTranspose3d):\n",
        "            module.weight = nn.init.kaiming_normal_(module.weight, a=self.neg_slope)\n",
        "            if module.bias is not None:\n",
        "                module.bias = nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            trunc_normal_(module.weight, std=self.neg_slope)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            nn.init.constant_(module.bias, 0)\n",
        "            nn.init.constant_(module.weight, 1.0)"
      ],
      "metadata": {
        "id": "udbIIsdNc0N3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fr unet"
      ],
      "metadata": {
        "id": "9j-10mmSc0Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, in_c, out_c, dp=0):\n",
        "        super(conv, self).__init__()\n",
        "        self.in_c = in_c\n",
        "        self.out_c = out_c\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.Dropout2d(dp),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.Dropout2d(dp),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class feature_fuse(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super(feature_fuse, self).__init__()\n",
        "        self.conv11 = nn.Conv2d(\n",
        "            in_c, out_c, kernel_size=1, padding=0, bias=False)\n",
        "        self.conv33 = nn.Conv2d(\n",
        "            in_c, out_c, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv33_di = nn.Conv2d(\n",
        "            in_c, out_c, kernel_size=3, padding=2, bias=False, dilation=2)\n",
        "        self.norm = nn.BatchNorm2d(out_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv11(x)\n",
        "        x2 = self.conv33(x)\n",
        "        x3 = self.conv33_di(x)\n",
        "        out = self.norm(x1+x2+x3)\n",
        "        return out\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_c, out_c, dp=0):\n",
        "        super(up, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_c, out_c, kernel_size=2,\n",
        "                               padding=0, stride=2, bias=False),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.LeakyReLU(0.1, inplace=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_c, out_c, dp=0):\n",
        "        super(down, self).__init__()\n",
        "        self.down = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, kernel_size=2,\n",
        "                      padding=0, stride=2, bias=False),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class block(nn.Module):\n",
        "    def __init__(self, in_c, out_c,  dp=0, is_up=False, is_down=False, fuse=False):\n",
        "        super(block, self).__init__()\n",
        "        self.in_c = in_c\n",
        "        self.out_c = out_c\n",
        "        if fuse == True:\n",
        "            self.fuse = feature_fuse(in_c, out_c)\n",
        "        else:\n",
        "            self.fuse = nn.Conv2d(in_c, out_c, kernel_size=1, stride=1)\n",
        "\n",
        "        self.is_up = is_up\n",
        "        self.is_down = is_down\n",
        "        self.conv = conv(out_c, out_c, dp=dp)\n",
        "        if self.is_up == True:\n",
        "            self.up = up(out_c, out_c//2)\n",
        "        if self.is_down == True:\n",
        "            self.down = down(out_c, out_c*2)\n",
        "\n",
        "    def forward(self,  x):\n",
        "        if self.in_c != self.out_c:\n",
        "            x = self.fuse(x)\n",
        "        x = self.conv(x)\n",
        "        if self.is_up == False and self.is_down == False:\n",
        "            return x\n",
        "        elif self.is_up == True and self.is_down == False:\n",
        "            x_up = self.up(x)\n",
        "            return x, x_up\n",
        "        elif self.is_up == False and self.is_down == True:\n",
        "            x_down = self.down(x)\n",
        "            return x, x_down\n",
        "        else:\n",
        "            x_up = self.up(x)\n",
        "            x_down = self.down(x)\n",
        "            return x, x_up, x_down\n",
        "\n",
        "\n",
        "class FR_UNet(nn.Module):\n",
        "    def __init__(self,  num_classes=1, num_channels=1, feature_scale=2,  dropout=0.2, fuse=True, out_ave=True):\n",
        "        super(FR_UNet, self).__init__()\n",
        "        self.out_ave = out_ave\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "        filters = [int(x / feature_scale) for x in filters]\n",
        "        self.block1_3 = block(\n",
        "            num_channels, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
        "        self.block1_2 = block(\n",
        "            filters[0], filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
        "        self.block1_1 = block(\n",
        "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
        "        self.block10 = block(\n",
        "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
        "        self.block11 = block(\n",
        "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
        "        self.block12 = block(\n",
        "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=False, fuse=fuse)\n",
        "        self.block13 = block(\n",
        "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=False, fuse=fuse)\n",
        "        self.block2_2 = block(\n",
        "            filters[1], filters[1],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
        "        self.block2_1 = block(\n",
        "            filters[1]*2, filters[1],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
        "        self.block20 = block(\n",
        "            filters[1]*3, filters[1],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
        "        self.block21 = block(\n",
        "            filters[1]*3, filters[1],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
        "        self.block22 = block(\n",
        "            filters[1]*3, filters[1],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
        "        self.block3_1 = block(\n",
        "            filters[2], filters[2],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
        "        self.block30 = block(\n",
        "            filters[2]*2, filters[2],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
        "        self.block31 = block(\n",
        "            filters[2]*3, filters[2],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
        "        self.block40 = block(filters[3], filters[3],\n",
        "                             dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
        "        self.final1 = nn.Conv2d(\n",
        "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
        "        self.final2 = nn.Conv2d(\n",
        "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
        "        self.final3 = nn.Conv2d(\n",
        "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
        "        self.final4 = nn.Conv2d(\n",
        "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
        "        self.final5 = nn.Conv2d(\n",
        "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
        "        self.fuse = nn.Conv2d(\n",
        "            5, num_classes, kernel_size=1, padding=0, bias=True)\n",
        "        self.apply(InitWeights_He)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1_3, x_down1_3 = self.block1_3(x)\n",
        "        x1_2, x_down1_2 = self.block1_2(x1_3)\n",
        "        x2_2, x_up2_2, x_down2_2 = self.block2_2(x_down1_3)\n",
        "        x1_1, x_down1_1 = self.block1_1(torch.cat([x1_2, x_up2_2], dim=1))\n",
        "        x2_1, x_up2_1, x_down2_1 = self.block2_1(\n",
        "            torch.cat([x_down1_2, x2_2], dim=1))\n",
        "        x3_1, x_up3_1, x_down3_1 = self.block3_1(x_down2_2)\n",
        "        x10, x_down10 = self.block10(torch.cat([x1_1, x_up2_1], dim=1))\n",
        "        x20, x_up20, x_down20 = self.block20(\n",
        "            torch.cat([x_down1_1, x2_1, x_up3_1], dim=1))\n",
        "        x30, x_up30 = self.block30(torch.cat([x_down2_1, x3_1], dim=1))\n",
        "        _, x_up40 = self.block40(x_down3_1)\n",
        "        x11, x_down11 = self.block11(torch.cat([x10, x_up20], dim=1))\n",
        "        x21, x_up21 = self.block21(torch.cat([x_down10, x20, x_up30], dim=1))\n",
        "        _, x_up31 = self.block31(torch.cat([x_down20, x30, x_up40], dim=1))\n",
        "        x12 = self.block12(torch.cat([x11, x_up21], dim=1))\n",
        "        _, x_up22 = self.block22(torch.cat([x_down11, x21, x_up31], dim=1))\n",
        "        x13 = self.block13(torch.cat([x12, x_up22], dim=1))\n",
        "        if self.out_ave == True:\n",
        "            output = (self.final1(x1_1)+self.final2(x10) +\n",
        "                      self.final3(x11)+self.final4(x12)+self.final5(x13))/5\n",
        "        else:\n",
        "            output = self.final5(x13)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "EYcKSIjUc0CM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "ej_6K_Tqcr2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomVerticalFlip\n",
        "# from utils.helpers import Fix_RandomRotation\n",
        "\n",
        "\n",
        "class vessel_dataset(Dataset):\n",
        "    def __init__(self, path, mode, is_val=False, split=None):\n",
        "\n",
        "        self.mode = mode\n",
        "        self.is_val = is_val\n",
        "        self.data_path = os.path.join(path, f\"{mode}_pro\")\n",
        "        self.data_file = os.listdir(self.data_path)\n",
        "        self.img_file = self._select_img(self.data_file)\n",
        "        if split is not None and mode == \"training\":\n",
        "            assert split > 0 and split < 1\n",
        "            if not is_val:\n",
        "                self.img_file = self.img_file[:int(split*len(self.img_file))]\n",
        "            else:\n",
        "                self.img_file = self.img_file[int(split*len(self.img_file)):]\n",
        "        self.transforms = Compose([\n",
        "            RandomHorizontalFlip(p=0.5),\n",
        "            RandomVerticalFlip(p=0.5),\n",
        "            Fix_RandomRotation(),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file = self.img_file[idx]\n",
        "        with open(file=os.path.join(self.data_path, img_file), mode='rb') as file:\n",
        "            img = torch.from_numpy(pickle.load(file)).float()\n",
        "        gt_file = \"gt\" + img_file[3:]\n",
        "        with open(file=os.path.join(self.data_path, gt_file), mode='rb') as file:\n",
        "            gt = torch.from_numpy(pickle.load(file)).float()\n",
        "\n",
        "        if self.mode == \"training\" and not self.is_val:\n",
        "            seed = torch.seed()\n",
        "            torch.manual_seed(seed)\n",
        "            img = self.transforms(img)\n",
        "            torch.manual_seed(seed)\n",
        "            gt = self.transforms(gt)\n",
        "\n",
        "        return img, gt\n",
        "\n",
        "    def _select_img(self, file_list):\n",
        "        img_list = []\n",
        "        for file in file_list:\n",
        "            if file[:3] == \"img\":\n",
        "                img_list.append(file)\n",
        "\n",
        "        return img_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_file)"
      ],
      "metadata": {
        "id": "09eqsvBt-gnP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataprocessor"
      ],
      "metadata": {
        "id": "k2ko1XTiazha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Grayscale, Normalize, ToTensor\n",
        "# from utils.helpers import dir_exists, remove_files\n",
        "\n",
        "\n",
        "def data_process(data_path, name, patch_size, stride, mode):\n",
        "    save_path = os.path.join(data_path, f\"{mode}_pro\")\n",
        "    dir_exists(save_path)\n",
        "    remove_files(save_path)\n",
        "    if name == \"DRIVE\":\n",
        "        # img_path = os.path.join(data_path, mode, \"images\")\n",
        "        # gt_path = os.path.join(data_path, mode, \"1st_manual\")\n",
        "        img_path = \"new_data/train/image/\"\n",
        "        gt_path = \"new_data/train/mask/\"\n",
        "        file_list = list(sorted(os.listdir(img_path)))\n",
        "    elif name == \"CHASEDB1\":\n",
        "        file_list = list(sorted(os.listdir(data_path)))\n",
        "    elif name == \"STARE\":\n",
        "        img_path = os.path.join(data_path, \"stare-images\")\n",
        "        gt_path = os.path.join(data_path, \"labels-ah\")\n",
        "        file_list = list(sorted(os.listdir(img_path)))\n",
        "    elif name == \"DCA1\":\n",
        "        data_path = os.path.join(data_path, \"Database_134_Angiograms\")\n",
        "        file_list = list(sorted(os.listdir(data_path)))\n",
        "    elif name == \"CHUAC\":\n",
        "        img_path = os.path.join(data_path, \"Original\")\n",
        "        gt_path = os.path.join(data_path, \"Photoshop\")\n",
        "        file_list = list(sorted(os.listdir(img_path)))\n",
        "    img_list = []\n",
        "    gt_list = []\n",
        "    for i, file in enumerate(file_list):\n",
        "        if name == \"DRIVE\":\n",
        "            img = Image.open(os.path.join(img_path, file))\n",
        "            # gt = Image.open(os.path.join(gt_path, file[0:2] + \"_manual1.gif\"))\n",
        "            gt = Image.open(os.path.join(gt_path, file))\n",
        "            img = Grayscale(1)(img)\n",
        "            img_list.append(ToTensor()(img))\n",
        "            gt_list.append(ToTensor()(gt))\n",
        "\n",
        "        elif name == \"CHASEDB1\":\n",
        "            if len(file) == 13:\n",
        "                if mode == \"training\" and int(file[6:8]) <= 10:\n",
        "                    img = Image.open(os.path.join(data_path, file))\n",
        "                    gt = Image.open(os.path.join(\n",
        "                        data_path, file[0:9] + '_1stHO.png'))\n",
        "                    img = Grayscale(1)(img)\n",
        "                    img_list.append(ToTensor()(img))\n",
        "                    gt_list.append(ToTensor()(gt))\n",
        "                elif mode == \"test\" and int(file[6:8]) > 10:\n",
        "                    img = Image.open(os.path.join(data_path, file))\n",
        "                    gt = Image.open(os.path.join(\n",
        "                        data_path, file[0:9] + '_1stHO.png'))\n",
        "                    img = Grayscale(1)(img)\n",
        "                    img_list.append(ToTensor()(img))\n",
        "                    gt_list.append(ToTensor()(gt))\n",
        "        elif name == \"DCA1\":\n",
        "            if len(file) <= 7:\n",
        "                if mode == \"training\" and int(file[:-4]) <= 100:\n",
        "                    img = cv2.imread(os.path.join(data_path, file), 0)\n",
        "                    gt = cv2.imread(os.path.join(\n",
        "                        data_path, file[:-4] + '_gt.pgm'), 0)\n",
        "                    gt = np.where(gt >= 100, 255, 0).astype(np.uint8)\n",
        "                    img_list.append(ToTensor()(img))\n",
        "                    gt_list.append(ToTensor()(gt))\n",
        "                elif mode == \"test\" and int(file[:-4]) > 100:\n",
        "                    img = cv2.imread(os.path.join(data_path, file), 0)\n",
        "                    gt = cv2.imread(os.path.join(\n",
        "                        data_path, file[:-4] + '_gt.pgm'), 0)\n",
        "                    gt = np.where(gt >= 100, 255, 0).astype(np.uint8)\n",
        "                    img_list.append(ToTensor()(img))\n",
        "                    gt_list.append(ToTensor()(gt))\n",
        "        elif name == \"CHUAC\":\n",
        "            if mode == \"training\" and int(file[:-4]) <= 20:\n",
        "                img = cv2.imread(os.path.join(img_path, file), 0)\n",
        "                if int(file[:-4]) <= 17 and int(file[:-4]) >= 11:\n",
        "                    tail = \"PNG\"\n",
        "                else:\n",
        "                    tail = \"png\"\n",
        "                gt = cv2.imread(os.path.join(\n",
        "                    gt_path, \"angio\"+file[:-4] + \"ok.\"+tail), 0)\n",
        "                gt = np.where(gt >= 100, 255, 0).astype(np.uint8)\n",
        "                img = cv2.resize(\n",
        "                    img, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
        "                cv2.imwrite(f\"save_picture/{i}img.png\", img)\n",
        "                cv2.imwrite(f\"save_picture/{i}gt.png\", gt)\n",
        "                img_list.append(ToTensor()(img))\n",
        "                gt_list.append(ToTensor()(gt))\n",
        "            elif mode == \"test\" and int(file[:-4]) > 20:\n",
        "                img = cv2.imread(os.path.join(img_path, file), 0)\n",
        "                gt = cv2.imread(os.path.join(\n",
        "                    gt_path, \"angio\"+file[:-4] + \"ok.png\"), 0)\n",
        "                gt = np.where(gt >= 100, 255, 0).astype(np.uint8)\n",
        "                img = cv2.resize(\n",
        "                    img, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
        "                cv2.imwrite(f\"save_picture/{i}img.png\", img)\n",
        "                cv2.imwrite(f\"save_picture/{i}gt.png\", gt)\n",
        "                img_list.append(ToTensor()(img))\n",
        "                gt_list.append(ToTensor()(gt))\n",
        "        elif name == \"STARE\":\n",
        "            if not file.endswith(\"gz\"):\n",
        "                img = Image.open(os.path.join(img_path, file))\n",
        "                gt = Image.open(os.path.join(gt_path, file[0:6] + '.ah.ppm'))\n",
        "                cv2.imwrite(f\"save_picture/{i}img.png\", np.array(img))\n",
        "                cv2.imwrite(f\"save_picture/{i}gt.png\", np.array(gt))\n",
        "                img = Grayscale(1)(img)\n",
        "                img_list.append(ToTensor()(img))\n",
        "                gt_list.append(ToTensor()(gt))\n",
        "    img_list = normalization(img_list)\n",
        "    if mode == \"training\":\n",
        "        img_patch = get_patch(img_list, patch_size, stride)\n",
        "        gt_patch = get_patch(gt_list, patch_size, stride)\n",
        "        save_patch(img_patch, save_path, \"img_patch\", name)\n",
        "        save_patch(gt_patch, save_path, \"gt_patch\", name)\n",
        "    elif mode == \"test\":\n",
        "        if name != \"CHUAC\":\n",
        "            img_list = get_square(img_list, name)\n",
        "            gt_list = get_square(gt_list, name)\n",
        "        save_each_image(img_list, save_path, \"img\", name)\n",
        "        save_each_image(gt_list, save_path, \"gt\", name)\n",
        "\n",
        "\n",
        "def get_square(img_list, name):\n",
        "    img_s = []\n",
        "    if name == \"DRIVE\":\n",
        "        shape = 592\n",
        "    elif name == \"CHASEDB1\":\n",
        "        shape = 1008\n",
        "    elif name == \"DCA1\":\n",
        "        shape = 320\n",
        "    _, h, w = img_list[0].shape\n",
        "    pad = nn.ConstantPad2d((0, shape-w, 0, shape-h), 0)\n",
        "    for i in range(len(img_list)):\n",
        "        img = pad(img_list[i])\n",
        "        img_s.append(img)\n",
        "\n",
        "    return img_s\n",
        "\n",
        "\n",
        "def get_patch(imgs_list, patch_size, stride):\n",
        "    image_list = []\n",
        "    _, h, w = imgs_list[0].shape\n",
        "    pad_h = stride - (h - patch_size) % stride\n",
        "    pad_w = stride - (w - patch_size) % stride\n",
        "    for sub1 in imgs_list:\n",
        "        image = F.pad(sub1, (0, pad_w, 0, pad_h), \"constant\", 0)\n",
        "        image = image.unfold(1, patch_size, stride).unfold(\n",
        "            2, patch_size, stride).permute(1, 2, 0, 3, 4)\n",
        "        image = image.contiguous().view(\n",
        "            image.shape[0] * image.shape[1], image.shape[2], patch_size, patch_size)\n",
        "        for sub2 in image:\n",
        "            image_list.append(sub2)\n",
        "    return image_list\n",
        "\n",
        "\n",
        "def save_patch(imgs_list, path, type, name):\n",
        "    for i, sub in enumerate(imgs_list):\n",
        "        with open(file=os.path.join(path, f'{type}_{i}.pkl'), mode='wb') as file:\n",
        "            pickle.dump(np.array(sub), file)\n",
        "            print(f'save {name} {type} : {type}_{i}.pkl')\n",
        "\n",
        "\n",
        "def save_each_image(imgs_list, path, type, name):\n",
        "    for i, sub in enumerate(imgs_list):\n",
        "        with open(file=os.path.join(path, f'{type}_{i}.pkl'), mode='wb') as file:\n",
        "            pickle.dump(np.array(sub), file)\n",
        "            print(f'save {name} {type} : {type}_{i}.pkl')\n",
        "\n",
        "\n",
        "def normalization(imgs_list):\n",
        "    imgs = torch.cat(imgs_list, dim=0)\n",
        "    mean = torch.mean(imgs)\n",
        "    std = torch.std(imgs)\n",
        "    normal_list = []\n",
        "    for i in imgs_list:\n",
        "        n = Normalize([mean], [std])(i)\n",
        "        n = (n - torch.min(n)) / (torch.max(n) - torch.min(n))\n",
        "        normal_list.append(n)\n",
        "    return normal_list\n",
        "\n",
        "\n",
        "DATA_PATH = 'datasets/DRIVE'\n",
        "DATA_NAME = \"DRIVE\"  # ['DRIVE','CHASEDB1','STARE','CHUAC','DCA1']\n",
        "PATCH_SIZE = 48  # the size of patch for image partition\n",
        "STRIDE = 6  # the stride of image partition\n",
        "\n",
        "\n",
        "data_process(DATA_PATH, DATA_NAME,\n",
        "              PATCH_SIZE, STRIDE, \"training\")\n",
        "# data_process(DATA_PATH, DATA_NAME,\n",
        "#               PATCH_SIZE, STRIDE, \"test\")"
      ],
      "metadata": {
        "id": "3MewFiE0azk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "OE3fdXjIazwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "Ml44eFfAazzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ttach"
      ],
      "metadata": {
        "id": "ldzTaicQif-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils import tensorboard\n",
        "from tqdm import tqdm\n",
        "# from utils.helpers import dir_exists, get_instance, remove_files, double_threshold_iteration\n",
        "# from utils.metrics import AverageMeter, get_metrics, get_metrics, count_connect_component\n",
        "import ttach as tta\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, CFG=None, loss=None, train_loader=None, val_loader=None):\n",
        "        self.CFG = CFG\n",
        "        if self.CFG.amp is True:\n",
        "            self.scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "        self.loss = loss\n",
        "        self.model = nn.DataParallel(model.cuda())\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.optimizer = get_instance(\n",
        "            torch.optim, \"optimizer\", CFG, self.model.parameters())\n",
        "        self.lr_scheduler = get_instance(\n",
        "            torch.optim.lr_scheduler, \"lr_scheduler\", CFG, self.optimizer)\n",
        "        start_time = datetime.now().strftime('%y%m%d%H%M%S')\n",
        "        self.checkpoint_dir = os.path.join(\n",
        "            CFG.save_dir, self.CFG['model']['type'], start_time)\n",
        "        self.writer = tensorboard.SummaryWriter(self.checkpoint_dir)\n",
        "        dir_exists(self.checkpoint_dir)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(1, self.CFG.epochs + 1):\n",
        "            self._train_epoch(epoch)\n",
        "            if self.val_loader is not None and epoch % self.CFG.val_per_epochs == 0:\n",
        "                results = self._valid_epoch(epoch)\n",
        "                # logger.info(f'## Info for epoch {epoch} ## ')\n",
        "                for k, v in results.items():\n",
        "                    # logger.info(f'{str(k):15s}: {v}')\n",
        "            if epoch % self.CFG.save_period == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "    def _train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        wrt_mode = 'train'\n",
        "        self._reset_metrics()\n",
        "        tbar = tqdm(self.train_loader, ncols=160)\n",
        "        tic = time.time()\n",
        "        for img, gt in tbar:\n",
        "            self.data_time.update(time.time() - tic)\n",
        "            img = img.cuda(non_blocking=True)\n",
        "            gt = gt.cuda(non_blocking=True)\n",
        "            self.optimizer.zero_grad()\n",
        "            if self.CFG.amp is True:\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    pre = self.model(img)\n",
        "                    loss = self.loss(pre, gt)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                pre = self.model(img)\n",
        "                loss = self.loss(pre, gt)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            self.total_loss.update(loss.item())\n",
        "            self.batch_time.update(time.time() - tic)\n",
        "\n",
        "            self._metrics_update(\n",
        "                *get_metrics(pre, gt, threshold=self.CFG.threshold).values())\n",
        "            tbar.set_description(\n",
        "                'TRAIN ({}) | Loss: {:.4f} | AUC {:.4f} F1 {:.4f} Acc {:.4f}  Sen {:.4f} Spe {:.4f} Pre {:.4f} IOU {:.4f} |B {:.2f} D {:.2f} |'.format(\n",
        "                    epoch, self.total_loss.average, *self._metrics_ave().values(), self.batch_time.average, self.data_time.average))\n",
        "            tic = time.time()\n",
        "        self.writer.add_scalar(\n",
        "            f'{wrt_mode}/loss', self.total_loss.average, epoch)\n",
        "        for k, v in list(self._metrics_ave().items())[:-1]:\n",
        "            self.writer.add_scalar(f'{wrt_mode}/{k}', v, epoch)\n",
        "        for i, opt_group in enumerate(self.optimizer.param_groups):\n",
        "            self.writer.add_scalar(\n",
        "                f'{wrt_mode}/Learning_rate_{i}', opt_group['lr'], epoch)\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "    def _valid_epoch(self, epoch):\n",
        "        logger.info('\\n###### EVALUATION ######')\n",
        "        self.model.eval()\n",
        "        wrt_mode = 'val'\n",
        "        self._reset_metrics()\n",
        "        tbar = tqdm(self.val_loader, ncols=160)\n",
        "        with torch.no_grad():\n",
        "            for img, gt in tbar:\n",
        "                img = img.cuda(non_blocking=True)\n",
        "                gt = gt.cuda(non_blocking=True)\n",
        "                if self.CFG.amp is True:\n",
        "                    with torch.cuda.amp.autocast(enabled=True):\n",
        "                        predict = self.model(img)\n",
        "                        loss = self.loss(predict, gt)\n",
        "                else:\n",
        "                    predict = self.model(img)\n",
        "                    loss = self.loss(predict, gt)\n",
        "                self.total_loss.update(loss.item())\n",
        "                self._metrics_update(\n",
        "                    *get_metrics(predict, gt, threshold=self.CFG.threshold).values())\n",
        "                tbar.set_description(\n",
        "                    'EVAL ({})  | Loss: {:.4f} | AUC {:.4f} F1 {:.4f} Acc {:.4f} Sen {:.4f} Spe {:.4f} Pre {:.4f} IOU {:.4f} |'.format(\n",
        "                        epoch, self.total_loss.average, *self._metrics_ave().values()))\n",
        "                self.writer.add_scalar(\n",
        "                    f'{wrt_mode}/loss', self.total_loss.average, epoch)\n",
        "\n",
        "        self.writer.add_scalar(\n",
        "            f'{wrt_mode}/loss', self.total_loss.average, epoch)\n",
        "        for k, v in list(self._metrics_ave().items())[:-1]:\n",
        "            self.writer.add_scalar(f'{wrt_mode}/{k}', v, epoch)\n",
        "        log = {\n",
        "            'val_loss': self.total_loss.average,\n",
        "            **self._metrics_ave()\n",
        "        }\n",
        "        return log\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        state = {\n",
        "            'arch': type(self.model).__name__,\n",
        "            'epoch': epoch,\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'config': self.CFG\n",
        "        }\n",
        "        filename = os.path.join(self.checkpoint_dir,\n",
        "                                f'checkpoint-epoch{epoch}.pth')\n",
        "        logger.info(f'Saving a checkpoint: {filename} ...')\n",
        "        torch.save(state, filename)\n",
        "        return filename\n",
        "\n",
        "    def _reset_metrics(self):\n",
        "        self.batch_time = AverageMeter()\n",
        "        self.data_time = AverageMeter()\n",
        "        self.total_loss = AverageMeter()\n",
        "        self.auc = AverageMeter()\n",
        "        self.f1 = AverageMeter()\n",
        "        self.acc = AverageMeter()\n",
        "        self.sen = AverageMeter()\n",
        "        self.spe = AverageMeter()\n",
        "        self.pre = AverageMeter()\n",
        "        self.iou = AverageMeter()\n",
        "        self.CCC = AverageMeter()\n",
        "    def _metrics_update(self, auc, f1, acc, sen, spe, pre, iou):\n",
        "        self.auc.update(auc)\n",
        "        self.f1.update(f1)\n",
        "        self.acc.update(acc)\n",
        "        self.sen.update(sen)\n",
        "        self.spe.update(spe)\n",
        "        self.pre.update(pre)\n",
        "        self.iou.update(iou)\n",
        "\n",
        "    def _metrics_ave(self):\n",
        "\n",
        "        return {\n",
        "            \"AUC\": self.auc.average,\n",
        "            \"F1\": self.f1.average,\n",
        "            \"Acc\": self.acc.average,\n",
        "            \"Sen\": self.sen.average,\n",
        "            \"Spe\": self.spe.average,\n",
        "            \"pre\": self.pre.average,\n",
        "            \"IOU\": self.iou.average\n",
        "        }"
      ],
      "metadata": {
        "id": "BSoszlZnaz26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Zo3Ccxk2az6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from bunch import Bunch\n",
        "# from loguru import logger\n",
        "# from ruamel.yaml import safe_load\n",
        "from torch.utils.data import DataLoader\n",
        "# import models\n",
        "# from dataset import vessel_dataset\n",
        "# from trainer import Trainer\n",
        "# from utils import losses\n",
        "# from utils.helpers import get_instance, seed_torch\n",
        "\n",
        "\n",
        "def main(CFG, data_path, batch_size, with_val=False):\n",
        "    seed_torch()\n",
        "    if with_val:\n",
        "        train_dataset = vessel_dataset(data_path, mode=\"training\", split=0.9)\n",
        "        val_dataset = vessel_dataset(\n",
        "            data_path, mode=\"training\", split=0.9, is_val=True)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size, shuffle=False, num_workers=16, pin_memory=True, drop_last=False)\n",
        "    else:\n",
        "        train_dataset = vessel_dataset(data_path, mode=\"training\")\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
        "\n",
        "    # logger.info('The patch number of train is %d' % len(train_dataset))\n",
        "    model = get_instance(models, 'model', CFG)\n",
        "    # logger.info(f'\\n{model}\\n')\n",
        "    loss = get_instance(losses, 'loss', CFG)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        loss=loss,\n",
        "        CFG=CFG,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader if with_val else None\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "DATA_PATH = \"\"\n",
        "BATCH_SIZE = 4\n",
        "VAL = True\n",
        "\n",
        "\n",
        "main(CFG, DATA_PATH, BATCH_SIZE, VAL)"
      ],
      "metadata": {
        "id": "qjXf9Jofaz-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}